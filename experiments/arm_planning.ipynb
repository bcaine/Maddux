{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tempfile\n",
    "import tensorflow as tf\n",
    "\n",
    "from tf_rl.controller import DiscreteDeepQ, HumanController\n",
    "from tf_rl import simulate\n",
    "from tf_rl.models import MLP\n",
    "from maddux.rl_experiments.simple_reward_planning import SimpleRewardPlanning\n",
    "from maddux.rl_experiments.environments import get_simple_environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tmp/tmpnRd_jW\n"
     ]
    }
   ],
   "source": [
    "LOG_DIR = tempfile.mkdtemp()\n",
    "print(LOG_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SAVE_DIR = \"/home/colin/robots/maddux/saved_experiments\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "game = SimpleRewardPlanning(get_simple_environment())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Passing a `GraphDef` to the SummaryWriter is deprecated. Pass a `Graph` object instead, such as `sess.graph`.\n"
     ]
    }
   ],
   "source": [
    "# Tensorflow business - it is always good to reset a graph before creating a new controller.\n",
    "tf.reset_default_graph()\n",
    "session = tf.InteractiveSession()\n",
    "\n",
    "# This little guy will let us run tensorboard\n",
    "#      tensorboard --logdir [LOG_DIR]\n",
    "journalist = tf.train.SummaryWriter(LOG_DIR)\n",
    "\n",
    "# Brain maps from observation to Q values for different actions.\n",
    "# Here it is a done using a multi layer perceptron with 2 hidden\n",
    "# layers\n",
    "brain = MLP([game.observation_size,], [200, 200, game.num_actions], \n",
    "            [tf.tanh, tf.tanh, tf.identity])\n",
    "\n",
    "# The optimizer to use. Here we use RMSProp as recommended\n",
    "# by the publication\n",
    "optimizer = tf.train.RMSPropOptimizer(learning_rate= 0.001, decay=0.9)\n",
    "\n",
    "# DiscreteDeepQ object\n",
    "current_controller = DiscreteDeepQ(game.observation_size, game.num_actions, brain, \n",
    "                                   optimizer, session, random_action_probability=1.0,\n",
    "                                   discount_rate=0.9, exploration_period=5000, \n",
    "                                   max_experience=10000, store_every_nth=1, \n",
    "                                   train_every_nth=1, summary_writer=journalist)\n",
    "\n",
    "session.run(tf.initialize_all_variables())\n",
    "session.run(current_controller.target_network_update)\n",
    "# graph was not available when journalist was created  \n",
    "journalist.add_graph(session.graph_def)\n",
    "\n",
    "game_idx = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Game 0:\n",
      "Iterations before end: 174.\n",
      "Random Prob: 1.000000\n",
      "Total Rewards: -101.73\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import matplotlib\n",
    "\n",
    "iterations_needed = []\n",
    "total_rewards = []\n",
    "\n",
    "try:\n",
    "    for game_idx in range(10000):\n",
    "        current_random_prob = current_controller.random_action_probability\n",
    "        update_random_prob = game_idx != 0 and game_idx % 50 == 0 and current_random_prob > 0.1\n",
    "        if update_random_prob:\n",
    "            current_controller.random_action_probability = current_random_prob - 0.05 \n",
    "        game = SimpleRewardPlanning(get_simple_environment())\n",
    "        game_iterations = 0\n",
    "\n",
    "        observation = game.observe()\n",
    "        while game_iterations < 500 and not game.is_over():\n",
    "            action = current_controller.action(observation)\n",
    "            reward = game.collect_reward(action)\n",
    "            new_observation = game.observe()\n",
    "            current_controller.store(observation, action, reward, new_observation)\n",
    "            current_controller.training_step()\n",
    "            observation = new_observation\n",
    "            game_iterations += 1\n",
    "        total_rewards.append(sum(game.collected_rewards))\n",
    "        iterations_needed.append(game_iterations)\n",
    "        rewards = []\n",
    "        if game_idx % 50 == 0:\n",
    "            print \"\\rGame %d:\\nIterations before end: %d.\" % (game_idx, game_iterations)\n",
    "            print \"Random Prob: %f\" % (current_controller.random_action_probability)\n",
    "            print \"Total Rewards: %s\\n\" % (sum(game.collected_rewards))\n",
    "            game.save_path(SAVE_DIR, game_idx)\n",
    "\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print \"Interrupted\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "plt.plot(total_rewards[0:500], label='Reward')\n",
    "plt.plot(iterations_needed[0:500], label='Iterations Needed')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 12))\n",
    "sns.jointplot(np.array(iterations_needed), np.array(total_rewards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 12))\n",
    "sns.jointplot(np.array(iterations_needed), np.array(total_rewards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
