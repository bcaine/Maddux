{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tempfile\n",
    "import tensorflow as tf\n",
    "\n",
    "from tf_rl.controller import DiscreteDeepQ, HumanController\n",
    "from tf_rl import simulate\n",
    "from tf_rl.models import MLP\n",
    "from maddux.rl_experiments.planning import Planning\n",
    "from maddux.rl_experiments.environments import get_simple_environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tmp/tmpRiJijn\n"
     ]
    }
   ],
   "source": [
    "LOG_DIR = tempfile.mkdtemp()\n",
    "print(LOG_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SAVE_DIR = \"/home/ben/Development/maddux/saved_experiments\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "game = Planning(get_simple_environment())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Tensorflow business - it is always good to reset a graph before creating a new controller.\n",
    "tf.reset_default_graph()\n",
    "session = tf.InteractiveSession()\n",
    "\n",
    "# This little guy will let us run tensorboard\n",
    "#      tensorboard --logdir [LOG_DIR]\n",
    "journalist = tf.train.SummaryWriter(LOG_DIR)\n",
    "\n",
    "# Brain maps from observation to Q values for different actions.\n",
    "# Here it is a done using a multi layer perceptron with 2 hidden\n",
    "# layers\n",
    "brain = MLP([game.observation_size,], [200, 200, game.num_actions], \n",
    "            [tf.tanh, tf.tanh, tf.identity])\n",
    "\n",
    "# The optimizer to use. Here we use RMSProp as recommended\n",
    "# by the publication\n",
    "optimizer = tf.train.RMSPropOptimizer(learning_rate= 0.001, decay=0.9)\n",
    "\n",
    "# DiscreteDeepQ object\n",
    "current_controller = DiscreteDeepQ(game.observation_size, game.num_actions, brain, \n",
    "                                   optimizer, session, discount_rate=0.90, \n",
    "                                   exploration_period=5000, max_experience=10000, \n",
    "                                   store_every_nth=4, train_every_nth=4,\n",
    "                                   summary_writer=journalist)\n",
    "\n",
    "session.run(tf.initialize_all_variables())\n",
    "session.run(current_controller.target_network_update)\n",
    "# graph was not available when journalist was created  \n",
    "journalist.add_graph(session.graph_def)\n",
    "\n",
    "game_idx = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game 635: iterations before success 28. Total Rewards: 4.86741582968 Last 5 rewards: [0.1899961688949352, 0.33462235032097531, 0.28486640293048271, 0.22076604846123782, 0.270252150307213]"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import matplotlib\n",
    "\n",
    "iterations_needed = []\n",
    "total_rewards = []\n",
    "\n",
    "try:\n",
    "    for game_idx in range(game_idx, 1000):\n",
    "        game = Planning(get_simple_environment())\n",
    "        game_iterations = 0\n",
    "\n",
    "        observation = game.observe()\n",
    "        while game_iterations < 1000 and not game.is_over():\n",
    "            action = current_controller.action(observation)\n",
    "            reward = game.collect_reward(action)\n",
    "            new_observation = game.observe()\n",
    "            current_controller.store(observation, action, reward, new_observation)\n",
    "            current_controller.training_step()\n",
    "            observation = new_observation\n",
    "            game_iterations += 1\n",
    "        total_rewards.append(sum(game.collected_rewards))\n",
    "        iterations_needed.append(game_iterations)\n",
    "        rewards = []\n",
    "        if game_idx % 5 == 0:\n",
    "            print \"\\rGame %d: iterations before success %d.\" % (game_idx, game_iterations),\n",
    "            print \"Total Rewards: %s\" % (sum(game.collected_rewards)),\n",
    "            print \"Last 5 rewards: {}\".format(game.collected_rewards[-5:]),\n",
    "            game.save_path(SAVE_DIR, game_idx)\n",
    "\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print \"Interrupted\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "plt.plot(total_rewards[0:500], label='Reward')\n",
    "plt.plot(iterations_needed[0:500], label='Iterations Needed')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 12))\n",
    "sns.jointplot(np.array(iterations_needed), np.array(total_rewards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
